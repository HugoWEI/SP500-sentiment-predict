{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gnews\n",
      "  Downloading gnews-0.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting feedparser~=6.0.2 (from gnews)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9.3 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gnews) (4.12.2)\n",
      "Collecting dnspython~=1.16.0 (from gnews)\n",
      "  Downloading dnspython-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gnews) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4<5,>=4.9.3->gnews) (2.4.1)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gnews) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gnews) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gnews) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hugow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->gnews) (2022.12.7)\n",
      "Downloading gnews-0.3.7-py3-none-any.whl (15 kB)\n",
      "Downloading dnspython-1.16.0-py2.py3-none-any.whl (188 kB)\n",
      "   ---------------------------------------- 0.0/188.4 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 30.7/188.4 kB 640.0 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 71.7/188.4 kB 777.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 112.6/188.4 kB 819.2 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 122.9/188.4 kB 798.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 143.4/188.4 kB 607.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 153.6/188.4 kB 610.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.4 kB 556.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 188.4/188.4 kB 541.4 kB/s eta 0:00:00\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/81.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 30.7/81.3 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 41.0/81.3 kB 487.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 71.7/81.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 504.3 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=cd904442002d58486bf2495d1bc0790db8554bff2885b0c4bd18b9eaabe9a775\n",
      "  Stored in directory: c:\\users\\hugow\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, dnspython, gnews\n",
      "Successfully installed dnspython-1.16.0 feedparser-6.0.11 gnews-0.3.7 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'BlackRock, Citadel Back Upstart Texas Challenge to NYSE, Nasdaq - Bloomberg', 'description': 'BlackRock, Citadel Back Upstart Texas Challenge to NYSE, Nasdaq  Bloomberg', 'published date': 'Wed, 05 Jun 2024 01:50:00 GMT', 'url': 'https://news.google.com/rss/articles/CBMicGh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDI0LTA2LTA1L2JsYWNrcm9jay1jaXRhZGVsLWJhY2stdXBzdGFydC10ZXhhcy1jaGFsbGVuZ2UtdG8tbnlzZS1uYXNkYXHSAQA?oc=5&hl=en-US&gl=US&ceid=US:en', 'publisher': {'href': 'https://www.bloomberg.com', 'title': 'Bloomberg'}}\n"
     ]
    }
   ],
   "source": [
    "from gnews import GNews\n",
    "\n",
    "\n",
    "site_cnbc = \"CNBC.com\"\n",
    "site_bloomberg = \"bloomberg.com\"\n",
    "test = GNews.get_news_by_site(self=GNews(), site=site_bloomberg)\n",
    "\n",
    "\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m FinancialNewsDataset(\u001b[43mnews_df\u001b[49m, tokenizer, max_len)\n\u001b[0;32m     35\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "class FinancialNewsDataset(Dataset):\n",
    "    def __init__(self, news_df, tokenizer, max_len):\n",
    "        self.news_df = news_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.news_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        headline = self.news_df.iloc[idx]['headline']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "max_len = 128\n",
    "batch_size = 16\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = FinancialNewsDataset(news_df, tokenizer, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load pre-trained FinBERT model for masked language modeling\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Set up optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(dataloader) * 3  # Number of epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}/{3}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_news_headlines(news_df, model, tokenizer, max_len):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for headline in news_df['headline']:\n",
    "            inputs = tokenizer.encode_plus(\n",
    "                headline,\n",
    "                max_length=max_len,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(cls_embedding)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "news_embeddings = embed_news_headlines(news_df, model, tokenizer, max_len)\n",
    "news_df['embedding'] = list(news_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP500Dataset(Dataset):\n",
    "    def __init__(self, news_df, price_df, n_days, max_len):\n",
    "        self.news_df = news_df\n",
    "        self.price_df = price_df\n",
    "        self.n_days = n_days\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.X, self.Y = self.create_features_labels()\n",
    "        \n",
    "    def create_features_labels(self):\n",
    "        X, Y = [], []\n",
    "        for i in range(self.n_days, len(self.price_df) - 1):\n",
    "            news_subset = self.news_df.loc[self.news_df['date'].isin(self.price_df['date'].iloc[i-self.n_days:i])]\n",
    "            if len(news_subset) < self.n_days:\n",
    "                continue  # Skip if there are not enough news articles\n",
    "\n",
    "            embeddings = np.vstack(news_subset['embedding'].values)\n",
    "            log_returns = self.price_df['log_return'].iloc[i-self.n_days:i].values\n",
    "\n",
    "            features = np.hstack([embeddings.flatten(), log_returns])\n",
    "            X.append(features)\n",
    "            \n",
    "            Y.append(self.price_df['log_return'].iloc[i + 1])\n",
    "        \n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    def __len__(\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
